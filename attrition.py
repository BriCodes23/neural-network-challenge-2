# -*- coding: utf-8 -*-
"""attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m4ZdVEBC2QxyuSNwd4rN8MJNnSztdSfp

## Part 1: Preprocessing
"""

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras import layers

#  Import and read the attrition data
attrition_df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m19/lms/datasets/attrition.csv')
attrition_df.head()

# Determine the number of unique values in each column.
attrition_df.nunique()

# Create y_df with the Attrition and Department columns
y_df = attrition_df[['Attrition', 'Department']]

# Create a list of at least 10 column names to use as X data
columns = [
    "Education",
    "Age",
    "DistanceFromHome",
    "JobSatisfaction",
    "OverTime",
    "StockOptionLevel",
    "WorkLifeBalance",
    "YearsAtCompany",
    "YearsSinceLastPromotion",
    "NumCompaniesWorked"
]

# Create X_df using your selected columns
X_df = attrition_df[columns]


# Show the data types for X_df
X_df.dtypes

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, random_state=78)

# Convert your X data to numeric data types however you see fit
X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)
# Add new code cells as necessary

# Create a StandardScaler
scaler = StandardScaler()

# Fit the StandardScaler to the training data
X_scaler = scaler.fit(X_train)

# Scale the training and testing data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create a OneHotEncoder for the Department column
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(sparse=False)


# Fit the encoder to the training data
enc.fit(y_train)


# Create two new variables by applying the encoder
# to the training and testing data
y_train_encoded = enc.transform(y_train)
y_test_encoded = enc.transform(y_test)

# Create a OneHotEncoder for the Attrition column
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(sparse=False)


# Fit the encoder to the training data
enc.fit(y_train)


# Create two new variables by applying the encoder
# to the training and testing data
y_train_encoded = enc.transform(y_train)
y_test_encoded = enc.transform(y_test)

"""## Create, Compile, and Train the Model"""

# Find the number of columns in the X training data
number_input_features = len(X_train.columns)
number_input_features


# Create the input layer
input = layers.Input(shape=(number_input_features,))


# Create at least two shared layers
layer1 = layers.Dense(units=10, activation="relu")(input)
layer2 = layers.Dense(units=5, activation="relu")(layer1)

# Create a branch for Department
# with a hidden layer and an output layer
department_input = layers.Input(shape=(len(X_train.columns),))
department_hidden = layers.Dense(units=10, activation="relu")(department_input)

# Create the hidden layer
department_hidden = layers.Dense(units=5, activation="relu")(department_hidden)

# Create the output layer
department_output = layers.Dense(units=1, activation="sigmoid")(department_hidden)

# Create a branch for Attrition
# with a hidden layer and an output layer
attrition_input = layers.Input(shape=(len(X_train.columns),))
attrition_hidden = layers.Dense(units=10, activation="relu")(attrition_input)

# Create the hidden layer
attrition_hidden = layers.Dense(units=5, activation="relu")(attrition_hidden)

# Create the output layer
attrition_output = layers.Dense(units=1, activation="sigmoid")(attrition_hidden)

# Create the model
model = Model(inputs=[input, department_input, attrition_input], outputs=[department_output, attrition_output])


# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


# Summarize the model
model.summary()

# Train the model
model.fit([X_train, y_train_encoded, y_train_encoded], [y_train_encoded, y_train_encoded], epochs=100, batch_size=32)

# Evaluate the model with the testing data
model_loss, model_accuracy = model.evaluate([X_test, y_test_encoded, y_test_encoded], [y_test_encoded, y_test_encoded])
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Print the accuracy for both department and attrition
print(f"Department Accuracy: {model_accuracy}")
print(f"Attrition Accuracy: {model_accuracy}")

"""# Summary

In the provided space below, briefly answer the following questions.

1. Is accuracy the best metric to use on this data? Why or why not?

2. What activation functions did you choose for your output layers, and why?

3. Can you name a few ways that this model might be improved?

YOUR ANSWERS HERE

1.
2.
3.
"""